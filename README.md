
# ğŸ½ï¸ Hadoop Ingredient Frequency Analysis

Analyze the most commonly used ingredients in a massive recipe dataset using **Hadoop MapReduce** running on **WSL (Windows Subsystem for Linux)**.

---

## ğŸ“˜ Project Overview

This project implements a Hadoop MapReduce job in Java to process a **2.3GB+ CSV recipe dataset**. The task is to identify and count the **most frequently used ingredients**, based on the column:
```
NER Ingredients but without amount, brand and such (Organized)
```

It is designed for coursework submission for the **Cloud Computing (EC7205)** module at the **University of Ruhuna**.

---

## ğŸ› ï¸ Tech Stack

- âœ… Windows 10/11 with WSL
- âœ… Ubuntu (via WSL)
- âœ… OpenJDK 11
- âœ… Apache Hadoop 2.10.2
- âœ… Java (MapReduce API)
- âœ… HDFS for input/output
- âœ… 2.3GB dataset from [Kaggle](https://www.kaggle.com/datasets/wilmerarltstrmberg/recipe-dataset-over-2m)

---

## ğŸš€ Step-by-Step Guide

### 1ï¸âƒ£ Install WSL + Ubuntu

In **PowerShell** (as admin):

```powershell
wsl --install
```

Then open Ubuntu from the Start menu and set up your username/password.

---

### 2ï¸âƒ£ Install Java

In the Ubuntu terminal:

```bash
sudo apt update
sudo apt install openjdk-11-jdk -y
java -version
```

---

### 3ï¸âƒ£ Download and Set Up Hadoop

```bash
cd ~
wget https://downloads.apache.org/hadoop/common/hadoop-2.10.2/hadoop-2.10.2.tar.gz
tar -xvzf hadoop-2.10.2.tar.gz
mv hadoop-2.10.2 hadoop
```

---

### 4ï¸âƒ£ Configure Environment Variables

Edit your `.bashrc`:

```bash
nano ~/.bashrc
```

Add:

```bash
export HADOOP_HOME=~/hadoop
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

Apply:

```bash
source ~/.bashrc
```

---

### 5ï¸âƒ£ Set JAVA_HOME in Hadoop

```bash
nano ~/hadoop/etc/hadoop/hadoop-env.sh
```

Set:

```bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

---

### 6ï¸âƒ£ Format HDFS and Start Hadoop

```bash
hdfs namenode -format
start-dfs.sh
start-yarn.sh
```

ğŸ’¡ If SSH fails:

```bash
sudo apt install openssh-server -y
ssh-keygen -t rsa -P ""
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
```

Then restart:

```bash
start-dfs.sh
start-yarn.sh
```

---

## ğŸ“‚ Project Structure

```bash
~/ingredient-count/
â”œâ”€â”€ IngredientFrequency.java     # MapReduce code
â”œâ”€â”€ ingredientcount.jar         # Compiled JAR
â”œâ”€â”€ top_ingredients.txt         # (optional) output
```

---

### 7ï¸âƒ£ Write the MapReduce Code

Create the source file:

```bash
cd ~
mkdir ingredient-count
cd ingredient-count
nano IngredientFrequency.java
```

ğŸ“Œ The full Java MapReduce code is provided in this repository as IngredientFrequency.java.

### 8ï¸âƒ£ Compile and Package

```bash
javac -classpath $(hadoop classpath) -d . IngredientFrequency.java
jar -cvf ingredientcount.jar *.class
```

---

## ğŸ“¥ Dataset

### 9ï¸âƒ£ Download Dataset from Kaggle

[Kaggle Recipe Dataset Over 2M](https://www.kaggle.com/datasets/wilmerarltstrmberg/recipe-dataset-over-2m)

Save it to:

```bash
/mnt/c/Users/YourUsername/Documents/Cloud/recipes_data.csv
```

Copy to WSL:

```bash
mkdir ~/datasets
cp "/mnt/c/Users/YourUsername/Documents/Cloud/recipes_data.csv" ~/datasets/
```

---

### ğŸ”Ÿ Upload to HDFS

```bash
hdfs dfs -mkdir -p /data_input
hdfs dfs -put ~/datasets/recipes_data.csv /data_input/
```

Verify:

```bash
hdfs dfs -ls /data_input
```

---

## â–¶ï¸ Run the MapReduce Job

```bash
hadoop jar ingredientcount.jar IngredientFrequency /data_input /output
```

If `/output` exists:

```bash
hdfs dfs -rm -r /output
```

---

### ğŸ“Š View Results

```bash
hdfs dfs -cat /output/part-r-00000 | sort -k2 -nr | head -20
```

Sample Output:
```
sugar     454288
onion     430754
eggs      425854
garlic    395201
butter    363527
...
```

---

## ğŸ“¤ Export Output File (Optional)

```bash
hdfs dfs -get /output/part-r-00000 ~/top_ingredients.txt
cp ~/top_ingredients.txt /mnt/c/Users/YourUsername/Documents/
```

---

## ğŸ” GitHub Setup (Optional)

```bash
cd ~/ingredient-count
git init
git add .
git commit -m "Initial commit for Hadoop ingredient frequency project"
git branch -M main
git remote add origin https://github.com/your-username/hadoop-ingredient-frequency.git
git push -u origin main
```

---

## ğŸ“ Credits

- Dataset from Kaggle (by Wilmer ArltstrÃ¶mberg)
- Developed as part of Cloud Computing Assignment (University of Ruhuna)
